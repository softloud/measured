

@misc{uoguelph_writing_2018,
	title = {Writing a {Literature} {Review} {\textbar} {Library}},
	url = {https://www.lib.uoguelph.ca/get-assistance/writing/specific-types-papers/writing-literature-review},
	urldate = {2018-11-27},
	author = {uoguelph},
	year = {2018},
	keywords = {literature review},
	file = {Writing a Literature Review | Library:/home/charles/Zotero/storage/MD2N9YAV/writing-literature-review.html:text/html}
}

@misc{writingcenter_literature_2018,
	title = {Literature {Reviews}},
	url = {https://writingcenter.unc.edu/tips-and-tools/literature-reviews/},
	abstract = {This handout will explain what literature reviews are and offer insights into the form and construction literature reviews.},
	language = {en-US},
	urldate = {2018-11-27},
	journal = {The Writing Center},
	author = {writingcenter},
	year = {2018},
	keywords = {literature review},
	file = {Snapshot:/home/charles/Zotero/storage/PECYXSAA/literature-reviews.html:text/html}
}

@book{marwick_rrtools:_2018,
	title = {rrtools: {Creates} a reproducible research compendium},
	url = {https://github.com/benmarwick/rrtools},
	author = {Marwick, Ben},
	year = {2018},
	annote = {R package version 0.1.0}
}

@book{blischak_workflowr:_2018,
	title = {workflowr: {A} {Framework} for {Reproducible} and {Collaborative} {Data} {Science}},
	url = {https://CRAN.R-project.org/package=workflowr},
	author = {Blischak, John and Carbonetto, Peter and Stephens, Matthew},
	year = {2018},
	annote = {R package version 1.1.1}
}

@article{marwick_packaging_2018,
	title = {Packaging {Data} {Analytical} {Work} {Reproducibly} {Using} {R} (and {Friends})},
	volume = {72},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2017.1375986},
	doi = {10.1080/00031305.2017.1375986},
	abstract = {Computers are a central tool in the research process, enabling complex and large-scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognizable way for organizing the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
	number = {1},
	urldate = {2018-11-24},
	journal = {The American Statistician},
	author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
	month = jan,
	year = {2018},
	keywords = {measured., Computational science, Data science, Open source software, Reproducible research, packaged data analysis, reproducibility},
	pages = {80--88},
	file = {Full Text PDF:/home/charles/Zotero/storage/MEBP88H9/Marwick et al. - 2018 - Packaging Data Analytical Work Reproducibly Using .pdf:application/pdf;Snapshot:/home/charles/Zotero/storage/K524VAIS/00031305.2017.html:text/html}
}
@article{edwards_bayesian_1963,
	title = {Bayesian statistical inference for psychological research},
	volume = {70},
	issn = {1939-1471(Electronic),0033-295X(Print)},
	doi = {10.1037/h0044139},
	abstract = {Bayesian statistics, a currently controversial viewpoint concerning statistical inference, is based on a definition of probability as a particular measure of the opinions of ideally consistent people. Statistical inference is modification of these opinions in the light of evidence, and Bayes' theorem specifies how such modifications should be made. The tools of Bayesian statistics include the theory of specific distributions and the principle of stable estimation, which specifies when actual prior opinions may be satisfactorily approximated by a uniform distribution. A common feature of many classical significance tests is that a sharp null hypothesis is compared with a diffuse alternative hypothesis. Often evidence which, for a Bayesian statistician, strikingly supports the null hypothesis leads to rejection of that hypothesis by standard classical procedures. The likelihood principle emphasized in Bayesian statistics implies, among other things, that the rules governing when data collection stops are irrelevant to data interpretation. It is entirely appropriate to collect data until a point has been proven or disproven, or until the data collector runs out of time, money, or patience. (71 ref.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Psychological Review},
	author = {Edwards, Ward and Lindman, Harold and Savage, Leonard J.},
	year = {1963},
	keywords = {Inference, Statistical Probability},
	pages = {193--242},
	file = {Snapshot:/home/charles/Zotero/storage/97ZQ3FEX/1964-00040-001.html:text/html}
}

@article{parker_opinionated_nodate,
	title = {Opinionated analysis development},
	url = {https://peerj.com/preprints/3210},
	doi = {10.7287/peerj.preprints.3210v1},
	abstract = {Traditionally, statistical training has focused primarily on mathematical derivations and proofs of statistical tests. The process of developing the technical artifact—that is, the paper, dashboard, or other deliverable—is much less frequently taught, presumably because of an aversion to cookbookery or prescribing specific software choices. In this paper I argue that it’s critical to teach analysts how to go about developing an analysis in order to maximize the probability that their analysis is reproducible, accurate, and collaborative. A critical component of this is adopting a blameless postmortem culture. By encouraging the use of and fluency in tooling that implements these opinions, as well as a blameless way of correcting course as analysts encounter errors, we as a community can foster the growth of processes that fail the practitioners as infrequently as possible.},
	language = {en},
	urldate = {2018-11-03},
	author = {Parker, Hilary},
	keywords = {measured.},
	file = {Parker - Opinionated analysis development.pdf:/home/charles/Zotero/storage/W2AAN92U/Parker - Opinionated analysis development.pdf:application/pdf}
}

@book{hester_covr:_2018,
	title = {covr: {Test} {Coverage} for {Packages}},
	url = {https://CRAN.R-project.org/package=covr},
	author = {Hester, Jim},
	year = {2018},
	keywords = {measured.},
	annote = {R package version 3.2.1}
}

@article{wilson_good_2017,
	title = {Good enough practices in scientific computing},
	volume = {13},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1005510},
	doi = {10.1371/journal.pcbi.1005510},
	language = {en},
	number = {6},
	urldate = {2018-11-17},
	journal = {PLOS Computational Biology},
	author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
	editor = {Ouellette, Francis},
	month = jun,
	year = {2017},
	keywords = {measured.},
	pages = {e1005510},
	file = {Wilson et al. - 2017 - Good enough practices in scientific computing.pdf:/home/charles/Zotero/storage/37F35SX5/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf:application/pdf}
}