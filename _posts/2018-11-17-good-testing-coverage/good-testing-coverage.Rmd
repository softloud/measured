---
title: "good enough testing coverage"
description: |
  What defines good testing coverage in an R analysis?
author:
  - name: Charles T. Gray
    url: {}
date: 11-16-2018
output:
  radix::radix_article:
    self_contained: false
bibliography: ../../biblio.bib
draft: true
categories: 
  - testing
tags: 
  - asssertr
  - validate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r packages}
# packages
library(varameta) # work in progress
library(validate)
library(tidyverse)
```

> this post is a work in progress


I've previously written about [how cool I think testing is](https://ropensci.org/blog/2018/03/13/ode-to-testing/), as well as [debugging](http://cantabile.rbind.io/posts/2018-11-12-testing-as-debugging/). It feels natural to next ask, when is one done? When has one achieved testing _coverage_? What does `covr`[@hester_covr:_2018] measure? What does the literature say about testing? My ultimate question, pedant as I am, what is _good enough_[@wilson_good_2017] testing?

## why do I care?

My first rstats projects were in bioinformatics, and it struck me how hands off and ad hoc the data analysis process was, given the cost of the research materials.   

## 

I want to test a function, `varameta::effect_se` from a package I'm working from. 

I'm pretty certain it takes log at times. 

```{r see function}
varameta:::g_lnorm
```

## what to test

Each function has a number of arguments. Each argument will have either a finite or infinite valid inputs, but will unlikely have infinite valid input data structures. 

In addition to data types, there is also the question of dimensionality. Some arguments must be length 1, and so forth.

So, we need to check that the function behaves as it should. 

We also need to check that the function _misbehaves_ as it should when passed something that it shouldn't.

But what exactly is _behave_ as it should? 

Well, we need to consider every possible valid input and combination thereof. 

### what to test in `varameta::effect_se`

This function takes arguments with specified defaults and some without. 

#### functions without defaults

argument | expected type | notes
-|-|-
`centre` | numeric | some of these estimators don't work with negative medians because $\log$
`spread` | numeric | estimators are designed to work with interquartile range or range
`n` | integer | sample size - still wondering if I should make this default to 1

#### functions with defaults

argument | default value  | expected type | notes
-|-|-|-
`centre_type` | `"mean"` | `"mean"` or `"median"` | check defaults are defaults
`spread_type` | `"iqr"` | `"iqr"` or `"range"` |

For these, we need to check defaults are defaults. Why? If I write tests that validate these defaults, then I'll know if I change the function and there is code that relies on it. 




## tools for testing

In Parker's rumination on _opinionated_ analysis, `assertr` and `validate` are noted as tools that can help with testing[@parker_opinionated_nodate]. How? 

### assertr

I'm unclear how to integrate `assertr::`[@fischetti_assertr:_2018] into `testthat::` and not sure I'm meant to. But it does seem like it could help me _assert_ that my assumptions about my pipe are correct.

> If any of these assertions were violated, an error would have been raised and the pipeline would have been terminated early[@fischetti_assertr:_2018].

I'm always doing this type of testing in the console; truncating my ` %>% ` and adding `dim` or `nrow` or something to the end. From my understanding of `assert`s I can build these tests into my pipes and functionally they act as the identify _if_ it passes the check. 

```{r}
# a generic example
iris %>% assertr::verify(nrow(.) %% 3 == 0) %>% head()

# an example from what I'm working on at the moment. 

# check the number of rows is divisible by the number of unique sample sizes. 
varameta::sim_par %>% assertr::verify(nrow(.) %% length(unique(.$sample_size)) == 0) %>% head()

```



### validate 

In the readme for [`validate::`](https://github.com/data-cleaning/validate), we have this description:

> The validate R-package makes it super-easy to check whether data lives up to expectations you have based on domain knowledge. It works by allowing you to define data validation rules independent of the code or data set.

What does _define data validation rules_ mean? 

> The validate package is intended to make checking your data easy, maintainable and reproducible[@loo_validate:_2018].

Still not entirely sure what _defining_ validation rules are, but this seems very neat. You can check the data is what you want it to be.  

For example, I require all true medians to be positive, as I'll take $\log$. 

```{r}
varameta::sim_par %>% validate::check_that(true_median > 0) %>% summary()

```

Well now, isn't that handy? 
